{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A world of infinite cheese\n",
    "class World(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cheese = False\n",
    "    \n",
    "    def step(self, eat):\n",
    "        if eat:\n",
    "            self.cheese = True\n",
    "        else:\n",
    "            self.cheese = False\n",
    "        \n",
    "        return self.cheese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mouse(object):\n",
    "    def __init__(self, value_learning_rate=0.1, action_learning_rate=0.01):\n",
    "        self._value_learning_rate = value_learning_rate\n",
    "        self._action_learning_rate = action_learning_rate\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.energy = self.initial_energy = 5\n",
    "        self.cheese = self.initial_cheese = None\n",
    "        self.action = self.initial_action = None\n",
    "        self.state = None\n",
    "        self.previous_state = None\n",
    "        \n",
    "        self._learned_value_table = {}\n",
    "        self._action_table = {}\n",
    "        for i in range(11):\n",
    "            for v in [True, False]:\n",
    "                k = (i, v)\n",
    "                self._learned_value_table[k] = 0\n",
    "                self._action_table[k] = 0.5\n",
    "    \n",
    "    @staticmethod\n",
    "    def _update_energy(energy, cheese):\n",
    "        if cheese:\n",
    "            energy += 1\n",
    "        else:\n",
    "            energy -= 1\n",
    "        \n",
    "        return np.clip(energy, 0, 10)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _innate_evaluation(current_state, full_point=7):\n",
    "        value = 0\n",
    "        energy, cheese = current_state\n",
    "        if cheese:\n",
    "            value = -energy + full_point\n",
    "        elif energy < 4:\n",
    "            value = energy - 4\n",
    "        return value\n",
    "    \n",
    "    def _update_learned_value_table(self, previous_state, value_difference, learning_rate, debug=True):\n",
    "        previous_value = self._learned_value_table[previous_state]\n",
    "        new_value = previous_value + learning_rate * value_difference\n",
    "        self._learned_value_table[previous_state] = new_value\n",
    "        if debug:\n",
    "            print(\"---- Update Learned Value Table\")\n",
    "            print(\"Previous State:\", previous_state)\n",
    "            print(\"Action:\", self.action)\n",
    "            print(\"Previous Value:\", previous_value)\n",
    "            print(\"Value Difference:\", value_difference)\n",
    "            print(\"New Value:\", new_value)\n",
    "\n",
    "    def _update_action_table(self, previous_state, value_difference, learning_rate, debug=True):\n",
    "        if self.action:\n",
    "            previous_strength = self._action_table[previous_state]\n",
    "            new_strength = previous_strength + learning_rate * value_difference\n",
    "            new_strength = np.clip(new_strength, 0.01, 0.99)\n",
    "            self._action_table[previous_state] = new_strength\n",
    "            if debug:\n",
    "                print(\"---- Update Action Table\")\n",
    "                print(\"Previous State:\", previous_state)\n",
    "                print(\"Previous Strength:\", previous_strength)\n",
    "                print(\"Value Difference:\", value_difference)\n",
    "                print(\"New Strength:\", new_strength)\n",
    "        else:\n",
    "            if debug:\n",
    "                print(\"Did not act last step. Nothing to change.\")\n",
    "        \n",
    "    def _learn(self, previous_state, value, debug=False):\n",
    "        value_difference = value - self._learned_value_table[previous_state]\n",
    "        self._update_learned_value_table(\n",
    "            previous_state, \n",
    "            value_difference, \n",
    "            self._value_learning_rate,\n",
    "            debug=debug\n",
    "        )\n",
    "        self._update_action_table(\n",
    "            previous_state, \n",
    "            value_difference, \n",
    "            self._action_learning_rate,\n",
    "            debug=debug\n",
    "        )\n",
    "    \n",
    "    def _get_action(self, cheese):\n",
    "        eat = False\n",
    "        act_chance = self._action_table[(self.energy, cheese)]\n",
    "        if np.random.random() < act_chance:\n",
    "            eat = True\n",
    "        return eat\n",
    "    \n",
    "    def _dict_to_table(self, d):\n",
    "        int_d = defaultdict(list)\n",
    "        for k, v in d.items():\n",
    "            p1, p2 = k\n",
    "            int_d[p1].append(v)\n",
    "        \n",
    "        return [[k] + v for k, v in int_d.items()]\n",
    "    \n",
    "    def _display_table(self, d_table, headers):\n",
    "        t = self._dict_to_table(d_table)\n",
    "        print()\n",
    "        print(tabulate(t, headers=headers))\n",
    "        print()\n",
    "    \n",
    "    def display_knowledge(self):\n",
    "        print(\"---- Learned Value Table\")\n",
    "        self._display_table(self._learned_value_table, headers=[\"Energy\", \"Cheese\", \"No Cheese\"])\n",
    "        print(\"---- Action Table\")\n",
    "        self._display_table(self._action_table, headers=[\"Energy\", \"Cheese\", \"No Cheese\"])\n",
    "    \n",
    "    def step(self, obseravation, debug=False):\n",
    "        cheese = observation # Boolean if the mouse tastes cheese or not\n",
    "        if debug: print(\"Initial energy value\", self.energy)\n",
    "        next_energy = self._update_energy(self.energy, cheese)\n",
    "        if debug: print(\"Updated energy value\", next_energy)\n",
    "        self.previous_state = self.state\n",
    "        self.state = (next_energy, cheese)\n",
    "        \n",
    "        if debug: \n",
    "            print(\"Previous State: (Energy, Cheese)\", self.previous_state)\n",
    "            print(\"Previous Action:\", self.action)\n",
    "            print(\"Current State: (Energy, Cheese)\", self.state)\n",
    "        if self.previous_state is not None:\n",
    "            if debug: print(\"Evaluating and learning ...\")\n",
    "            v_innate = self._innate_evaluation(self.state)\n",
    "            if debug: print(\"Innate Value:\", v_innate)\n",
    "            v_learned = self._learned_value_table[self.state]\n",
    "            if debug: print(\"Learned Value:\", v_learned)\n",
    "            value = v_innate + v_learned\n",
    "            if debug: print(\"Total Value:\", value)\n",
    "            self._learn(self.previous_state, value, debug)\n",
    "            if debug: self.display_knowledge()\n",
    "        \n",
    "        next_action = self._get_action(cheese)\n",
    "        if debug: \n",
    "            print(\"Action Chosen:\", next_action)\n",
    "            print(\"=\" * 50)\n",
    "        self.action = next_action\n",
    "        self.cheese = cheese\n",
    "        self.energy = next_energy\n",
    "        return self.action\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "world = World()\n",
    "mouse = Mouse(value_learning_rate=0.1, action_learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "mouse.reset()\n",
    "observation = False # No cheese to begin with\n",
    "for i in range(100):\n",
    "    action = mouse.step(observation, debug=False)\n",
    "    observation = world.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Learned Value Table\n",
      "\n",
      "  Energy     Cheese    No Cheese\n",
      "--------  ---------  -----------\n",
      "       0   0          2.16722\n",
      "       1   0.570196   0.604757\n",
      "       2   0.911228  -0.106198\n",
      "       3   0.366582   0.0117421\n",
      "       4   0.281248   0.0786253\n",
      "       5   0.272385   0.0221485\n",
      "       6   0          6.2857e-05\n",
      "       7  -0.1        6.2857e-06\n",
      "       8  -0.2       -0.3438\n",
      "       9  -0.0562     0\n",
      "      10   0          0\n",
      "\n",
      "---- Action Table\n",
      "\n",
      "  Energy    Cheese    No Cheese\n",
      "--------  --------  -----------\n",
      "       0  0.5          0.748361\n",
      "       1  0.612418     0.605742\n",
      "       2  0.593849     0.564713\n",
      "       3  0.561733     0.54638\n",
      "       4  0.532109     0.520927\n",
      "       5  0.513524     0.5\n",
      "       6  0.5          0.5\n",
      "       7  0.495        0.5\n",
      "       8  0.49         0.4809\n",
      "       9  0.5          0.5\n",
      "      10  0.5          0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mouse.display_knowledge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
